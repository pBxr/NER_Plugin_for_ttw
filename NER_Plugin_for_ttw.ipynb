{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a457f1e-8d0f-4eb1-83ba-5d64b28dc371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class log_NER_Class:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.actualTime = time.localtime()\n",
    "        self.year, self.month, self.day = self.actualTime[0:3]\n",
    "        self.hour, self.minute, self.second = self.actualTime[3:6]\n",
    "        \n",
    "        self.logCollector = []\n",
    "        \n",
    "        self.logCollector.append(f\"Log file: {self.year:4d}-{self.month:02d}-{self.day:02d}_{self.hour:02d}:{self.minute:02d}:{self.second:02d}\\n\")\n",
    "        \n",
    "    def add_to_log(self, logInput):\n",
    "        \n",
    "        self.logCollector.append(logInput)\n",
    "        \n",
    "    def save_results(self, resultJSONList, resultForCSVList):\n",
    "        \n",
    "        files.projectPath+\"\\\\NER_results\\\\\"\n",
    "        \n",
    "        #First the log\n",
    "        with open(files.projectPath+\"\\\\NER_results\\\\\"+\"01_log.txt\", 'w', encoding=\"utf8\") as fp:\n",
    "        \n",
    "            for logEntry in self.logCollector:\n",
    "                fp.write(logEntry)\n",
    "                print(logEntry)\n",
    "            fp.close()\n",
    "        \n",
    "        #Now the .csv list\n",
    "        intro=\"Place name|Suggested ID\\nNote|\\\"(*NOT LIKELY*)\\\" means, that the entity is not of type \\\"populated-place\\\", \\\"archaeological-site\\\" or \\\"archaeological-area\\\".\\n\"    \n",
    "    \n",
    "        resultForCSVList_sorted = sorted(resultForCSVList)\n",
    "\n",
    "        with open(files.projectPath+\"\\\\NER_results\\\\\"+\"02_Gazetteer_IDs_DRAFT.csv\", 'w', encoding=\"utf8\") as fp:\n",
    "            fp.write(intro + \"\\n\")\n",
    "            print(intro)\n",
    "            for item in resultForCSVList_sorted:    \n",
    "                fp.write(item + \"\\n\")\n",
    "                print(item)\n",
    "            fp.close()\n",
    "        \n",
    "        #Now the complete .json file\n",
    "        with open(files.projectPath+\"\\\\NER_results\\\\\"+\"03_Gazetteer_result_detailed.json\", 'w', encoding=\"utf8\") as fp:\n",
    "            resultJSON = json.dumps(resultJSONList,\n",
    "                          indent=4, sort_keys=False,\n",
    "                          separators=(',', ': '), ensure_ascii=False)\n",
    "            fp.write(resultJSON)\n",
    "            fp.close()\n",
    "\n",
    "            \n",
    "def call_gazetteer(results, logGenerator):\n",
    "\n",
    "    listGazetteer = []\n",
    "    listForCSV = []\n",
    "    csvRow = \"\"\n",
    "    logGenerator.add_to_log(\"\\n3. iDAI.gazetteer query result\")\n",
    "    \n",
    "    for result in results:\n",
    "        \n",
    "        toBeRun = filter_NER_results(result) #Decides whether the entry will be run or not\n",
    "        \n",
    "        if toBeRun == True:\n",
    "            #Most simple way to call gazetteer, only for testing purposes, more elaborated filters following. \n",
    "            #See also the README.md file on this point.\n",
    "            toSearch = \"https://gazetteer.dainst.org/search.json?q=\" + result\n",
    "            response = requests.get(toSearch)\n",
    "            resultListComplete = response.json()\n",
    "                         \n",
    "            logGenerator.add_to_log(f\"\\n--------------------------------------------------------------\\nSearching in iDAI.gazetteer for \\\"{result}\\\"\\n\")\n",
    "            \n",
    "            logGenerator.add_to_log(f\"Number of results: {resultListComplete['total']}\\n\")\n",
    "            \n",
    "            resultList = resultListComplete['result']\n",
    "            i=1\n",
    "\n",
    "            for item in resultList:\n",
    "                if item['prefName']['title']:\n",
    "                    logGenerator.add_to_log(f\"Nr. {i}: Preferred Name: {item['prefName']['title']}\\n\")\n",
    "                \n",
    "                if \"types\" in item:\n",
    "                    logGenerator.add_to_log(\"Type: \")\n",
    "                    \n",
    "                    for entry in item['types']:\n",
    "                        logGenerator.add_to_log(entry +\", \")\n",
    "                    logGenerator.add_to_log(\"\\n\")\n",
    "                \n",
    "                if \"@id\" in item:\n",
    "                    logGenerator.add_to_log(item['@id'])\n",
    "                \n",
    "                if item['prefName']['title'] and \"@id\" in item:\n",
    "                    \n",
    "                    if (\"types\" in item) and ('archaeological-area' in item['types']\n",
    "                        or 'populated-place' in item['types']\n",
    "                        or 'archaeological-site' in item['types']):\n",
    "                        \n",
    "                        csvRow = result + \"|\" + item['@id']\n",
    "                        logGenerator.add_to_log(\"\\n\")\n",
    "                        listForCSV.append(csvRow)\n",
    "                    else:\n",
    "                        result2 = result + \"(*NOT LIKELY*)\"\n",
    "                        csvRow = result2 + \"|\" + item['@id']\n",
    "                        logGenerator.add_to_log(\"\\n\")\n",
    "                        \n",
    "                        listForCSV.append(csvRow) #To save only the needed entries\n",
    "                i+=1\n",
    "            \n",
    "            logGenerator.add_to_log(\"\\n--------------------------------------------------------------\\n\")        \n",
    "            toSearch=\"\"\n",
    "            \n",
    "            listGazetteer.append(resultListComplete) #To save the complete result\n",
    "\n",
    "    return listGazetteer, listForCSV\n",
    "\n",
    "\n",
    "def filter_NER_results(result):\n",
    "    \"\"\"\n",
    "    This is only a simple placeholder for a more elaborated function.\n",
    "    \"\"\"\n",
    "    if len(result) > 3:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def prepare_folder_and_input_text(files, settings):    \n",
    "\n",
    "    #Prepare folder\n",
    "    pathNERresults = files.projectPath + \"NER_results\"\n",
    "\n",
    "    if not os.path.exists(pathNERresults):\n",
    "        os.makedirs(pathNERresults)\n",
    "\n",
    "    #Convert text to the selected input format\n",
    "    if settings.NER_SettingsSet['Source'] == 'Convert .docx to .txt and get text':\n",
    "        pandocParameter = \"00_Plain_article_text.txt\"\n",
    "    else:\n",
    "        pandocParameter = \"00_Plain_article_text.html\"\n",
    "\n",
    "    #Put together the pandoc call to convert the .docx file into the selected format and save it\n",
    "    pandocCall = \"pandoc -o \" + \"\\\"\" + pathNERresults + \"\\\\\" + pandocParameter + \"\\\"\" + \" \" + \"\\\"\" + files.projectPath + files.fileName + \"\\\"\"\n",
    "\n",
    "    FNULL = open(os.devnull, 'w') #For subprocess\n",
    "    subprocess.run(pandocCall, stdout=FNULL, stderr=FNULL, shell=False)\n",
    "\n",
    "    #Return the plain text for the pipeline. If a structured format like .html is selected, text gets extracted with bs4.\n",
    "    plainTextPath = pathNERresults + \"\\\\\" + pandocParameter\n",
    "\n",
    "    if settings.NER_SettingsSet['Source'] == 'Convert .docx to .txt and get text':\n",
    "        with open(plainTextPath, 'r', encoding=\"utf8\") as fp:\n",
    "            inputText = fp.read()\n",
    "            fp.close()\n",
    "        return inputText\n",
    "    else:\n",
    "        with open(plainTextPath, 'r', encoding=\"utf8\") as fp:\n",
    "            soup = BeautifulSoup(fp, \"html.parser\")\n",
    "            text = soup.get_text()\n",
    "            #Remove blank lines\n",
    "            inputText = str(text).replace('\\n\\n','')\n",
    "            return inputText\n",
    "\n",
    "\n",
    "def return_location_names(nerResults, logGenerator):\n",
    "    \n",
    "    listNames = []\n",
    "    logGenerator.add_to_log(\"\\n1. NER result:\\n\")\n",
    "    for result in nerResults:\n",
    "            logGenerator.add_to_log(str(result)+\"\\n\")\n",
    "    \n",
    "    for entry in nerResults:\n",
    "        #\"I-LOC\" and \"B-LOC\" are specific for the selected model. If you use another model check the entity types.\n",
    "        if entry['entity'] == \"I-LOC\" or entry['entity'] == \"B-LOC\":\n",
    "            \n",
    "            if \"##\" in entry['word']:\n",
    "                toInsert = entry['word'].replace(\"##\", \"\")\n",
    "            else:\n",
    "                toInsert = \"%%\"+ entry['word']\n",
    "            listNames.append(toInsert)\n",
    "            \n",
    "    result = ''.join(listNames)\n",
    "    locationNamesRaw = result.split(\"%%\")\n",
    "    locationNamesRaw.remove('')\n",
    "    locationNames = list(set(locationNamesRaw))\n",
    "    \n",
    "    logGenerator.add_to_log(\"\\n2. Extracted entities de-tokenized\\n\")\n",
    "    for entry in locationNames:\n",
    "        logGenerator.add_to_log(entry +\", \")\n",
    "    logGenerator.add_to_log(\"\\n\")\n",
    "    return locationNames\n",
    "\n",
    "\n",
    "def run_NER_process(files, settings):\n",
    "    \n",
    "    logGenerator = log_NER_Class()\n",
    "    \n",
    "    inputText = prepare_folder_and_input_text(files, settings)\n",
    "\n",
    "    selectedModell = settings.NER_SettingsSet['Model']\n",
    "    \n",
    "    #Now run NER\n",
    "    tokenizer = AutoTokenizer.from_pretrained(selectedModell)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(selectedModell)\n",
    "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "        \n",
    "    nerResults = nlp(inputText)\n",
    "    \n",
    "    #Now extract names, get iDAI.gazetteer entries and save log and results\n",
    "    extractedLocationNames = return_location_names(nerResults, logGenerator)\n",
    "    resultJSONList, resultForCSVList = call_gazetteer(extractedLocationNames, logGenerator)\n",
    "    logGenerator.save_results(resultJSONList, resultForCSVList)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "The following two classes resp. instances simulate the input from `ttw` into this plugin. \n",
    "The idea is that the plugin gets called with\n",
    "- the project path \n",
    "- the file name of the article\n",
    "- and the settings (selected model, entity type and method of text extraction)\n",
    "\n",
    "For the testing purposes the settings are hard coded here.\n",
    "An instance of each class containing the settings will be passed as arg to \"run_NER_process(files, settings)\"\n",
    "\"\"\"\n",
    "\n",
    "class files_class:\n",
    "    def __init__(self):\n",
    "        self.projectPath = \"C:\\\\#enter_your_project_path_here#\\\\\"\n",
    "        self.fileName = \"#enter_your_file_name_here#\"\n",
    "\n",
    "class settings_class:\n",
    "    def __init__(self):\n",
    "        self.NER_SettingsSet = {'Model' : 'dslim/bert-base-NER',\n",
    "                               'Entity Type' : 'Place Name',\n",
    "                               'Source' : 'Convert .docx to .txt and get text'\n",
    "                             }\n",
    "\n",
    "files = files_class()\n",
    "settings = settings_class()\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    #This ist the way the plugin will be called by `ttw` including the args\n",
    "    run_NER_process(files, settings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
